{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76fbaf88-5952-47bf-a68c-85011e49b6de",
   "metadata": {},
   "source": [
    "# Building our First RAG bot - Skill: talk to Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967c3b06-c8a0-45db-be9a-974c762ba4b8",
   "metadata": {},
   "source": [
    "We have now all the building blocks to build our first Bot that \"talks with my data\". These blocks are:\n",
    "\n",
    "1) A well indexed hybrid (text and vector) engine with my data in chunks -> Azure AI Search\n",
    "2) A good LLM python framework to build LLM Apps -> LangChain\n",
    "3) Quality OpenAI GPT models that understand language and follow instructions -> GPT3.5 and GPT4\n",
    "4) A persisten memory database -> CosmosDB\n",
    "\n",
    "We are missing just one thing: **Agents**.\n",
    "\n",
    "In this Notebook we introduce the concept of Agents and we use it to build or first RAG bot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b64f701d-5b9d-4c7c-b259-c2a515c75961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": ["import os\nimport random\nimport asyncio\nfrom typing import Dict, List\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import Optional, Type\n\nfrom langchain.agents import AgentExecutor, create_openai_tools_agent\nfrom langchain_openai import AzureChatOpenAI\nfrom langchain_core.runnables import ConfigurableField, ConfigurableFieldSpec\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_core.chat_history import BaseChatMessageHistory\nfrom langchain_community.chat_message_histories import ChatMessageHistory, CosmosDBChatMessageHistory\n\n\nfrom langchain_core.callbacks import CallbackManagerForToolRun\n\nfrom langchain_core.callbacks import AsyncCallbackManagerForToolRun\nfrom pydantic import BaseModel, Field\n\n\nfrom langchain_core.tools import BaseTool\n\nfrom langchain_core.tools import StructuredTool\n\nfrom langchain_core.tools import tool\n\n#custom libraries that we will use later in the app\nfrom common.utils import  GetDocSearchResults_Tool\nfrom common.prompts import AGENT_DOCSEARCH_PROMPT\n\nfrom IPython.display import Markdown, HTML, display  \n\ndef printmd(string):\n    display(Markdown(string))\n\nfrom dotenv import load_dotenv\nload_dotenv(\"credentials.env\")\n"]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4163af7-39d0-43b4-8dad-c13108d22a1d",
   "metadata": {},
   "outputs": [],
   "source": ["# Set the ENV variables that Langchain needs to connect to Azure OpenAI\nos.environ[\"OPENAI_API_VERSION\"] = os.environ[\"AZURE_OPENAI_API_VERSION\"]"]
  },
  {
   "cell_type": "markdown",
   "id": "33836104-822e-4846-8b81-0de8e24838f1",
   "metadata": {},
   "source": [
    "## Introducing: Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fc3d38-93f8-4a47-8125-d1bb9f529178",
   "metadata": {},
   "source": [
    "The implementation of Agents is inspired by two papers: the [MRKL Systems](https://arxiv.org/abs/2205.00445) paper (pronounced â€˜miracleâ€™ ðŸ˜‰) and the [ReAct](https://arxiv.org/abs/2210.03629) paper.\n",
    "\n",
    "Agents are a way to leverage the ability of LLMs to understand and act on prompts. In essence, an Agent is an LLM that has been given a very clever initial prompt. The prompt tells the LLM to break down the process of answering a complex query into a sequence of steps that are resolved one at a time.\n",
    "\n",
    "Agents become really cool when we combine them with â€˜expertsâ€™, introduced in the MRKL paper. Simple example: an Agent might not have the inherent capability to reliably perform mathematical calculations by itself. However, we can introduce an expert - in this case a calculator, an expert at mathematical calculations. Now, when we need to perform a calculation, the Agent can call in the expert rather than trying to predict the result itself. This is actually the concept behind [ChatGPT Pluggins](https://openai.com/blog/chatgpt-plugins).\n",
    "\n",
    "In our case, in order to solve the problem \"How do I build a smart bot that talks to my data\", we need this REACT/MRKL approach, in which we need to instruct the LLM that it needs to use 'experts/tools' in order to read/load/understand/interact with a any particular source of data.\n",
    "\n",
    "Let's create then an Agent that interact with the user and uses a Tool to get the information from the Search engine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7999a06-aff0-4d21-8be7-fe56c70082a8",
   "metadata": {},
   "source": [
    "#### 1. We start first defining the Tool/Expert\n",
    "\n",
    "Tools are functions that an agent can invoke. If you don't give the agent access to a correct set of tools, it will never be able to accomplish the objectives you give it. If you don't describe the tools well, the agent won't know how to use them properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a862366b-ce9e-44f8-9610-84ec568653ea",
   "metadata": {},
   "outputs": [],
   "source": ["index1_name = \"srch-index-usecases\"\nindex2_name = \"srch-index-csv\"\nindex3_name = \"srch-index-books\"\nindexes = [index1_name, index2_name, index3_name]"]
  },
  {
   "cell_type": "markdown",
   "id": "077886c8-c5d0-481d-a5f9-f4becf60e0f9",
   "metadata": {},
   "source": [
    "We have to convert the Retreiver object into a Tool object (\"the expert\"). Check out the Tool `GetDocSearchResults_Tool` in `utils.py` and see how it is done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73c6ca7-d93b-4961-b90a-08572cad78d8",
   "metadata": {},
   "source": [
    "Declare the tools the agent will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a0fd3a0-527c-42e3-a092-46e03d33bd07",
   "metadata": {},
   "outputs": [],
   "source": ["tools = [GetDocSearchResults_Tool(indexes=indexes, k=5, reranker_th=1, sas_token='?'+ os.environ['BLOB_SAS_TOKEN'])]"]
  },
  {
   "cell_type": "markdown",
   "id": "5f3ddf18-3f3c-44b4-8af5-1437973da010",
   "metadata": {},
   "source": [
    "#### 2. Define the LLM to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5aaaf7f5-ef26-48d8-868d-b53aa4c4f9f4",
   "metadata": {},
   "outputs": [],
   "source": ["COMPLETION_TOKENS = 1500\nllm = AzureChatOpenAI(deployment_name=os.environ[\"GPT35_DEPLOYMENT_NAME\"], \n                      temperature=0.5, max_tokens=COMPLETION_TOKENS, streaming=True)"]
  },
  {
   "cell_type": "markdown",
   "id": "d865755b-e4bb-468a-8dcc-4ac1999782b3",
   "metadata": {},
   "source": [
    "#### 3. Bind tools to the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec61b209-1c1e-48ff-957e-1ec2e375ada4",
   "metadata": {},
   "source": [
    "Newer OpenAI models (1106 and newer) have been fine-tuned to detect when one or more function(s) should be called and respond with the inputs that should be passed to the function(s). In an API call, you can describe functions and have the model intelligently choose to output a JSON object containing arguments to call these functions. The goal of the OpenAI tools APIs is to more reliably return valid and useful function calls than what can be done using a generic text completion or chat API.\n",
    "\n",
    "OpenAI termed the capability to invoke a single function as **functions**, and the capability to invoke one or more functions as [**tools**](https://platform.openai.com/docs/guides/function-calling).\n",
    "\n",
    "> OpenAI API has deprecated functions in favor of tools. The difference between the two is that the tools API allows the model to request that multiple functions be invoked at once, which can reduce response times in some architectures. Itâ€™s recommended to use the tools agent for OpenAI models.\n",
    "\n",
    "Having an LLM call multiple tools at the same time can greatly speed up agents whether there are tasks that are assisted by doing so. Thankfully, OpenAI models versions 1106 and newer support parallel function calling, which we will need to make sure our smart bot is performant.\n",
    "\n",
    "##### **From now on and for the rest of the notebooks, we are going to use OpenAI tools API tool call our experts/tools**\n",
    "\n",
    "To pass in our tools to the agent, we just need to format them to the [OpenAI tool format](https://platform.openai.com/docs/api-reference/chat/create) and pass them to our model. (By bind-ing the functions, weâ€™re making sure that theyâ€™re passed in each time the model is invoked.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "856361f5-87b5-46f0-a0a6-ce3c1566ff48",
   "metadata": {},
   "outputs": [],
   "source": ["# Bind (attach) the tools/functions we want on each LLM call\n\nllm_with_tools = llm.bind_tools(tools)\n\n# Let's also add the option to configure in real time the model we want\n\nllm_with_tools = llm_with_tools.configurable_alternatives(\n    ConfigurableField(id=\"model\"),\n    default_key=\"gpt35\",\n    gpt4=AzureChatOpenAI(deployment_name=os.environ[\"GPT4_DEPLOYMENT_NAME\"], temperature=0.5, max_tokens=COMPLETION_TOKENS, streaming=True),\n    gpt4o=AzureChatOpenAI(deployment_name=os.environ[\"GPT4_DEPLOYMENT_NAME\"], temperature=0.5, max_tokens=COMPLETION_TOKENS, streaming=True) \n)"]
  },
  {
   "cell_type": "markdown",
   "id": "330c64bd-89ca-494e-8c01-f948f9a3e6a7",
   "metadata": {},
   "source": [
    "#### 4. Define the System Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30901f95-3bf9-4aaa-9eda-226edbf5ea00",
   "metadata": {},
   "source": [
    "Because OpenAI Function Calling is finetuned for tool usage, we hardly need any instructions on how to reason, or how to output format. We will just have two input variables: `question` and `agent_scratchpad`. The input variable `question` should be a string containing the user objective, and `agent_scratchpad` should be a sequence of messages that contains the previous agent tool invocations and the corresponding tool outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cac295-8be5-4803-8342-6d4e48cd2294",
   "metadata": {},
   "source": [
    "Get the prompt to use `AGENT_DOCSEARCH_PROMPT` - you can modify this in `prompts.py`! Check it out!\n",
    "It looks like this:\n",
    "\n",
    "```python\n",
    "AGENT_DOCSEARCH_PROMPT = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", CUSTOM_CHATBOT_PREFIX  + DOCSEARCH_PROMPT_TEXT),\n",
    "        MessagesPlaceholder(variable_name='history', optional=True),\n",
    "        (\"human\", \"{question}\"),\n",
    "        MessagesPlaceholder(variable_name='agent_scratchpad')\n",
    "    ]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a44f8df6-a68e-4215-99f3-10119f796c0c",
   "metadata": {},
   "outputs": [],
   "source": ["prompt = AGENT_DOCSEARCH_PROMPT"]
  },
  {
   "cell_type": "markdown",
   "id": "581ad422-c06b-434f-bff0-e2a3d6093932",
   "metadata": {},
   "source": [
    "#### 5. Create the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3519b70c-007d-405c-9a81-18f58c5617be",
   "metadata": {},
   "source": [
    "The core idea of agents is to use a language model to choose a sequence of actions to take. In chains, a sequence of actions is hardcoded (in code). In agents, a language model is used as a reasoning engine to determine which actions to take and in which order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16be0ef1-dc72-49fa-8aa7-cdd2153ef8b1",
   "metadata": {},
   "outputs": [],
   "source": ["from langchain.agents.format_scratchpad.openai_tools import format_to_openai_tool_messages\nfrom langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser\n\nagent = (\n    {\n        \"question\": lambda x: x[\"question\"],\n        \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(x[\"intermediate_steps\"]),\n    }\n    | prompt\n    | llm_with_tools\n    | OpenAIToolsAgentOutputParser()\n)"]
  },
  {
   "cell_type": "markdown",
   "id": "d87d9a8b-2a93-4250-b1dc-b124fa8c7ffa",
   "metadata": {},
   "source": [
    "Or , which is equivalent, LangChain has a class that does exactly the cell code above: `create_openai_tools_agent`\n",
    "\n",
    "```python\n",
    "agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "```\n",
    "\n",
    "**Important Note: Other models like Mistral Large or Command R+ won't work with the same OpenAI Tools API, so in order to create agents with these models, try using the ReAct type instead from langchain**. Like [THIS COHERE AGENT](https://python.langchain.com/docs/integrations/providers/cohere/#react-agent) for example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338336d9-a64a-4602-908a-742b418e4520",
   "metadata": {},
   "source": [
    "Create an agent executor by passing in the agent and tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad6c156f-9a17-4daa-80de-70ce2f55063b",
   "metadata": {},
   "outputs": [],
   "source": ["agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=False)"]
  },
  {
   "cell_type": "markdown",
   "id": "252a017c-3b36-43ab-8633-78f4f005d166",
   "metadata": {},
   "source": [
    "Give it memory - since AgentExecutor is also a Runnable class, we do the same with did on Notebook 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c013314-afe6-4218-b179-d0f7312d2670",
   "metadata": {},
   "outputs": [],
   "source": ["def get_session_history(session_id: str, user_id: str) -> CosmosDBChatMessageHistory:\n    cosmos = CosmosDBChatMessageHistory(\n        cosmos_endpoint=os.environ['AZURE_COSMOSDB_ENDPOINT'],\n        cosmos_database=os.environ['AZURE_COSMOSDB_NAME'],\n        cosmos_container=os.environ['AZURE_COSMOSDB_CONTAINER_NAME'],\n        connection_string=os.environ['AZURE_COMOSDB_CONNECTION_STRING'],\n        session_id=session_id,\n        user_id=user_id\n        )\n\n    # prepare the cosmosdb instance\n    cosmos.prepare_cosmos()\n    return cosmos"]
  },
  {
   "cell_type": "markdown",
   "id": "13df017f-3ab7-4943-adc1-3477badf3d3e",
   "metadata": {},
   "source": [
    "Because cosmosDB needs two fields (an id and a partition), and RunnableWithMessageHistory takes by default only one identifier for memory (session_id), we need to use `history_factory_config` parameter and define the multiple keys for the memory class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf93758f-da3b-48fb-9882-91fe327b1751",
   "metadata": {},
   "outputs": [],
   "source": ["userid_spec = ConfigurableFieldSpec(\n            id=\"user_id\",\n            annotation=str,\n            name=\"User ID\",\n            description=\"Unique identifier for the user.\",\n            default=\"\",\n            is_shared=True,\n        )\nsession_id = ConfigurableFieldSpec(\n            id=\"session_id\",\n            annotation=str,\n            name=\"Session ID\",\n            description=\"Unique identifier for the conversation.\",\n            default=\"\",\n            is_shared=True,\n        )"]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52d1aaa6-efca-4512-b680-896dae39a359",
   "metadata": {},
   "outputs": [],
   "source": ["agent_with_chat_history = RunnableWithMessageHistory(\n    agent_executor,\n    get_session_history,\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n    history_factory_config=[userid_spec,session_id]\n)"]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "05c6b489-3db9-4965-9eae-ed2790e62bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'session_id': 'session339', 'user_id': 'user232'}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": ["# configure the session id and user id\nrandom_session_id = \"session\"+ str(random.randint(1, 1000))\nramdom_user_id = \"user\"+ str(random.randint(1, 1000))\n\nconfig={\"configurable\": {\"session_id\": random_session_id, \"user_id\": ramdom_user_id}}\nconfig"]
  },
  {
   "cell_type": "markdown",
   "id": "3295c54e-a5e2-46f6-99fc-6f76453a877d",
   "metadata": {},
   "source": [
    "#### 6.Run the Agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb3fca7e-33a1-40f1-afb0-dee441a1d1d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "StyleTTS2 currently uses HiFi-GAN as one of its decoder options for waveform generation. It offers two types of decoders: one based on HiFi-GAN and another based on iSTFTNet. The HiFi-GAN-based decoder directly generates waveforms, while the iSTFTNet-based decoder uses inverse short-time Fourier transform for waveform synthesis [[1]](https://blobstorageffpanhhmq7wy3.blob.core.windows.net/books/StyleTTS2.pdf?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2024-09-13T16:22:30Z&st=2024-09-13T08:22:30Z&spr=https&sig=mnoIDbS7Z3ooFM%2BVbIFU9TbrIGfG14ss1Lt5HD1l3io%3D).\n",
       "\n",
       "As for BigVGAN, it is a universal vocoder designed for high-fidelity and out-of-distribution audio generation. It introduces periodic activations and an anti-aliased multi-periodicity composition module to improve waveform modeling and reduce high-frequency artifacts. BigVGAN has been shown to outperform other neural vocoders like HiFi-GAN in various scenarios, including zero-shot generation for unseen speakers and novel environments [[2]](https://blobstorageffpanhhmq7wy3.blob.core.windows.net/books/BigVGAN.pdf?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2024-09-13T16:22:30Z&st=2024-09-13T08:22:30Z&spr=https&sig=mnoIDbS7Z3ooFM%2BVbIFU9TbrIGfG14ss1Lt5HD1l3io%3D).\n",
       "\n",
       "There is no direct indication that StyleTTS2 uses BigVGAN instead of HiFi-GAN, but theoretically, it could be possible to integrate BigVGAN as a vocoder for StyleTTS2, given its advantages in certain scenarios. However, this would require specific implementation efforts to adapt the integration."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": ["printmd(agent_with_chat_history.invoke(\n    {\"question\": \"Could StyleTTS2 Use BigVGAN instead of HiFiGan i think maybe StarVGAN uses it?\"}, \n    config=config)[\"output\"])"]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c430c456-f390-4319-a3b1-bee19da130cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "BigVGAN, a vocoding GAN, is designed for high-fidelity audio synthesis in the text-to-speech (TTS) domain. Here are some key aspects of BigVGAN's application in TTS:\n",
       "\n",
       "### Key Features and Innovations\n",
       "\n",
       "1. **High-Fidelity Out-of-Distribution Generation**: BigVGAN can generate high-quality audio without fine-tuning, even for out-of-distribution scenarios like unseen speakers, novel languages, singing voices, and varied recording environments [[1]](https://blobstorageffpanhhmq7wy3.blob.core.windows.net/books/BigVGAN.pdf?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2024-09-13T16:22:30Z&st=2024-09-13T08:22:30Z&spr=https&sig=mnoIDbS7Z3ooFM%2BVbIFU9TbrIGfG14ss1Lt5HD1l3io%3D).\n",
       "\n",
       "2. **Periodic Activations**: The generator uses periodic activations, such as the Snake function, which provides the necessary inductive bias for audio synthesis. This improves extrapolation capabilities, making the model robust to audio samples not seen during training [[2]](https://blobstorageffpanhhmq7wy3.blob.core.windows.net/books/BigVGAN.pdf?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2024-09-13T16:22:30Z&st=2024-09-13T08:22:30Z&spr=https&sig=mnoIDbS7Z3ooFM%2BVbIFU9TbrIGfG14ss1Lt5HD1l3io%3D).\n",
       "\n",
       "3. **Anti-Aliased Multi-Periodicity Composition (AMP) Module**: This module models complex audio waveforms by composing multiple signal components with learnable periodicities and using low-pass filters to reduce high-frequency artifacts [[3]](https://blobstorageffpanhhmq7wy3.blob.core.windows.net/books/BigVGAN.pdf?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2024-09-13T16:22:30Z&st=2024-09-13T08:22:30Z&spr=https&sig=mnoIDbS7Z3ooFM%2BVbIFU9TbrIGfG14ss1Lt5HD1l3io%3D).\n",
       "\n",
       "4. **Scalability**: BigVGAN scales up to 112M parameters, outperforming state-of-the-art models in zero-shot generation tasks. It is designed to maintain stability during large-scale GAN training [[4]](https://blobstorageffpanhhmq7wy3.blob.core.windows.net/books/BigVGAN.pdf?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2024-09-13T16:22:30Z&st=2024-09-13T08:22:30Z&spr=https&sig=mnoIDbS7Z3ooFM%2BVbIFU9TbrIGfG14ss1Lt5HD1l3io%3D).\n",
       "\n",
       "### Training and Architecture\n",
       "\n",
       "- **Training Data**: BigVGAN is trained on the LibriTTS dataset, utilizing diverse recording environments to enhance its universal vocoding capabilities [[5]](https://blobstorageffpanhhmq7wy3.blob.core.windows.net/books/BigVGAN.pdf?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2024-09-13T16:22:30Z&st=2024-09-13T08:22:30Z&spr=https&sig=mnoIDbS7Z3ooFM%2BVbIFU9TbrIGfG14ss1Lt5HD1l3io%3D).\n",
       "\n",
       "- **Generator Architecture**: The generator uses a stack of residual blocks with dilated convolutions and incorporates the AMP module to enhance feature refinement [[6]](https://blobstorageffpanhhmq7wy3.blob.core.windows.net/books/BigVGAN.pdf?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2024-09-13T16:22:30Z&st=2024-09-13T08:22:30Z&spr=https&sig=mnoIDbS7Z3ooFM%2BVbIFU9TbrIGfG14ss1Lt5HD1l3io%3D).\n",
       "\n",
       "BigVGAN's advancements in TTS demonstrate its potential for generating high-quality, realistic audio across a wide range of applications."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": ["printmd(agent_with_chat_history.invoke(\n        {\"question\": \"Interesting, Tell me more about the use of BigVGAN, specifically in the TTS domain?\"},\n        config=config)[\"output\"])"]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9fd54f71-03c9-4332-885b-0d1df942fa88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "You're welcome! If you have any more questions or need assistance with anything else, feel free to ask."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 168 ms, sys: 0 ns, total: 168 ms\n",
      "Wall time: 4.38 s\n"
     ]
    }
   ],
   "source": ["%%time\nprintmd(agent_with_chat_history.invoke({\"question\": \"Thhank you!\"}, config=config)[\"output\"])"]
  },
  {
   "cell_type": "markdown",
   "id": "149648ba-945d-4e7d-81f7-a8bca2ac87f2",
   "metadata": {},
   "source": [
    "#### Important: there is a limitation of GPT3.5, once we start adding long prompts, and long contexts and thorough answers, or the agent makes multiple searches for multi-step questions, we run out of space (number of tokens)!\n",
    "\n",
    "You can minimize this by:\n",
    "- Shorter System Prompt\n",
    "- Smaller chunks (less than the default of 5000 characters)\n",
    "- Reducing topK to bring less relevant chunks\n",
    "\n",
    "However, you ultimately are sacrificing quality to make everything work with GPT3.5 (cheaper and faster model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41787714-73fd-4336-85f2-bec3abb41eda",
   "metadata": {},
   "source": [
    "### Let's add more things we have learned so far: dynamic LLM selection of GPT4 and asyncronous streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1511d2c3-97fe-4232-a560-014d0f157008",
   "metadata": {},
   "outputs": [],
   "source": ["agent = create_openai_tools_agent(llm_with_tools.with_config(configurable={\"model\": \"gpt4o\"}), tools, prompt) # We select now GPT-4o\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=False)\nagent_with_chat_history = RunnableWithMessageHistory(agent_executor,get_session_history,input_messages_key=\"question\", \n                                                     history_messages_key=\"history\", history_factory_config=[userid_spec,session_id])"]
  },
  {
   "cell_type": "markdown",
   "id": "7bec5b32-6017-44b9-97e7-34ba3695e688",
   "metadata": {},
   "source": [
    "In prior notebooks with use the function `.stream()` of the runnable in order to stream the tokens. However if you need to stream individual tokens from the agent or surface steps occuring within tools, you would need to use a combination of `Callbacks` and `.astream()` OR the new `astream_events` API (beta).\n",
    "\n",
    "Letâ€™s use here the astream_events API to stream the following events:\n",
    "\n",
    "    Agent Start with inputs\n",
    "    Tool Start with inputs\n",
    "    Tool End with outputs\n",
    "    Stream the agent final anwer token by token\n",
    "    Agent End with outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9600a35e-8d2e-43d0-a334-092b2e8b832c",
   "metadata": {},
   "outputs": [],
   "source": ["QUESTION = \"Tell me more about your last answer, search again multiple times and provide a deeper explanation\""]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3808fa33-05bb-4f5d-9ab9-7159f6db62a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting agent: AgentExecutor\n",
      "--\n",
      "Starting tool: docsearch with inputs: {'query': 'BigVGAN TTS domain features'}\n",
      "--\n",
      "Starting tool: docsearch with inputs: {'query': 'BigVGAN comparison with other vocoders'}\n",
      "--\n",
      "Starting tool: docsearch with inputs: {'query': 'BigVGAN architecture and training'}\n",
      "Done tool: docsearch\n",
      "--\n",
      "Done tool: docsearch\n",
      "--\n",
      "Done tool: docsearch\n",
      "--\n",
      "BigVGAN is a universal neural vocoder designed for high-fidelity audio synthesis, particularly in the text-to-speech (TTS) domain. Here's a deeper look into its features, architecture, and comparisons:\n",
      "\n",
      "### Key Features and Innovations\n",
      "\n",
      "1. **High-Fidelity Out-of-Distribution Generation**: BigVGAN excels in generating high-quality audio for out-of-distribution (OOD) scenarios, such as unseen speakers, languages, and recording environments, without requiring fine-tuning [[1]](https://blobstorageffpanhhmq7wy3.blob.core.windows.net/books/BigVGAN.pdf).\n",
      "\n",
      "2. **Periodic Activations**: The generator employs periodic activations like the Snake function, which provides the necessary inductive bias for audio synthesis. This helps the model handle diverse audio samples more effectively [[2]](https://blobstorageffpanhhmq7wy3.blob.core.windows.net/books/BigVGAN.pdf).\n",
      "\n",
      "3. **Anti-Aliased Multi-Periodicity Composition (AMP) Module**: This module models complex audio waveforms by composing multiple signal components with learnable periodicities and applying low-pass filters to reduce high-frequency artifacts [[3]](https://blobstorageffpanhhmq7wy3.blob.core.windows.net/books/BigVGAN.pdf).\n",
      "\n",
      "4. **Scalability**: BigVGAN scales up to 112M parameters, outperforming state-of-the-art models in zero-shot generation tasks. It maintains stability during large-scale GAN training [[4]](https://blobstorageffpanhhmq7wy3.blob.core.windows.net/books/BigVGAN.pdf).\n",
      "\n",
      "### Architecture and Training\n",
      "\n",
      "- **Generator Architecture**: BigVGAN's generator uses transposed 1-D convolutions followed by the AMP module. It integrates periodic inductive bias and anti-aliased feature representation for robust audio synthesis [[5]](https://blobstorageffpanhhmq7wy3.blob.core.windows.net/books/BigVGAN.pdf).\n",
      "\n",
      "- **Training Data**: The model is trained on the LibriTTS dataset, utilizing a diverse range of recording environments to enhance its universal vocoding capabilities [[6]](https://blobstorageffpanhhmq7wy3.blob.core.windows.net/books/BigVGAN.pdf).\n",
      "\n",
      "- **Training Objectives**: The training objectives include adversarial loss, feature matching loss, and spectral regression loss, similar to HiFi-GAN but with modifications to improve performance [[7]](https://blobstorageffpanhhmq7wy3.blob.core.windows.net/books/BigVGAN.pdf).\n",
      "\n",
      "### Comparison with Other Vocoders\n",
      "\n",
      "- **Performance**: BigVGAN outperforms HiFi-GAN and other vocoders in both objective and subjective metrics, especially in zero-shot scenarios involving unseen languages and environments [[8]](https://blobstorageffpanhhmq7wy3.blob.core.windows.net/books/BigVGAN.pdf).\n",
      "\n",
      "- **Robustness**: It demonstrates strong robustness and extrapolation capabilities due to its architectural innovations, which help in handling diverse and challenging audio conditions [[9]](https://blobstorageffpanhhmq7wy3.blob.core.windows.net/books/BigVGAN.pdf).\n",
      "\n",
      "- **Evaluation**: BigVGAN achieves higher mean opinion scores (MOS) and similarity mean opinion scores (SMOS) compared to other models, indicating better audio quality and closer resemblance to ground-truth recordings [[10]](https://blobstorageffpanhhmq7wy3.blob.core.windows.net/books/BigVGAN.pdf).\n",
      "\n",
      "Overall, BigVGAN represents a significant advancement in the TTS domain, offering high-quality, versatile audio synthesis across a wide range of conditions.\n",
      "--\n",
      "Done agent: AgentExecutor\n"
     ]
    }
   ],
   "source": ["async for event in agent_with_chat_history.astream_events(\n    {\"question\": QUESTION}, config=config, version=\"v1\",\n):\n    kind = event[\"event\"]\n    if kind == \"on_chain_start\":\n        if (event[\"name\"] == \"AgentExecutor\"):\n            print( f\"Starting agent: {event['name']}\")\n    elif kind == \"on_chain_end\":\n        if (event[\"name\"] == \"AgentExecutor\"):  \n            print()\n            print(\"--\")\n            print(f\"Done agent: {event['name']}\")\n    if kind == \"on_chat_model_stream\":\n        content = event[\"data\"][\"chunk\"].content\n        # Empty content in the context of OpenAI means that the model is asking for a tool to be invoked.\n        # So we only print non-empty content\n        if content:\n            print(content, end=\"\")\n    elif kind == \"on_tool_start\":\n        print(\"--\")\n        print(f\"Starting tool: {event['name']} with inputs: {event['data'].get('input')}\")\n    elif kind == \"on_tool_end\":\n        print(f\"Done tool: {event['name']}\")\n        # print(f\"Tool output was: {event['data'].get('output')}\")\n        print(\"--\")"]
  },
  {
   "cell_type": "markdown",
   "id": "4b41bba7-18df-4ab8-b4f6-60368160d348",
   "metadata": {},
   "source": [
    "#### Note: Try to run this last question with GPT3.5 and see how you are going to run out of token space in the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ec64bf-fe24-42fc-8dde-4d478f0af21e",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "We just built our first RAG BOT!.\n",
    "\n",
    "- We learned that **Agents + Tools are the best way to go about building Bots**. <br>\n",
    "- We converted the Azure Search retriever into a Tool using the function `GetDocSearchResults_Tool` in `utils.py`\n",
    "- We learned about the events API (Beta), one way to stream the answer from agents\n",
    "- We learned that for comprehensive, quality answers we will run out of space with GPT3.5. GPT4 then becomes necessary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56306506-d53d-4d43-93e2-a9300ed2a3ee",
   "metadata": {},
   "source": [
    "# NEXT\n",
    "\n",
    "Now that we have a bot with one skill (Document Search), let's build more skills!. In the next Notebook, we are going to build an agent that can understand tabular data in csv file and can execute python commands"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py310_sdkv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
