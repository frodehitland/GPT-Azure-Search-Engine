{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60ec6048-44e4-4118-b16a-9c4c9cc78a3b",
   "metadata": {},
   "source": [
    "# How to deal with complex/large Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9281ac79-47cd-49d4-bdd4-7f5c173a947d",
   "metadata": {},
   "source": [
    "In the previous notebook, we developed a solution for various types of files and data formats commonly found in organizations, and this covers big majority of the use cases. However, you will find that there are issues when dealing with questions that require answers from complex files. The complexity of these files arises from their length and the way information is distributed within them. Large documents are always a challenge for Search Engines.\n",
    "\n",
    "One example of such complex files is Technical Specification Guides or Product Manuals, which can span hundreds of pages and contain information in the form of images, tables, forms, and more. Books are also complex due to their length and the presence of images or tables.\n",
    "\n",
    "These files are typically in PDF format. To better handle these PDFs, we need a smarter parsing method that treats each document as a special source and processes them page by page (1 page = 1 chunk). The objective is to obtain more accurate and faster answers from our system. Fortunately, there are usually not many of these types of documents in an organization, allowing us to make exceptions and treat them differently.\n",
    "\n",
    "If your use case is just PDFs, for example, you can just use [PyPDF library](https://pypi.org/project/pypdf/) or [Azure AI Document Intelligence SDK (former Form Recognizer)](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/overview?view=doc-intel-3.0.0), vectorize using OpenAI API and push the content to a vector-based index. And this is problably the simplest and fastest way to go.  However if your use case entails connecting to a datalake, or Sharepoint libraries or any other document data source with thousands of documents with multiple file types and that can change dynamically, then you would want to use the Ingestion and Document Cracking and AI-Enrichment capabilities of Azure Search engine, Notebooks 1-3, and avoid a lot of painful custom code. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15f6044e-463f-4988-bc46-a3c3d641c15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "from common.utils import parse_pdf, read_pdf_files, text_to_base64\n",
    "from common.prompts import DOCSEARCH_PROMPT\n",
    "from common.utils import CustomAzureSearchRetriever\n",
    "\n",
    "\n",
    "from IPython.display import Markdown, HTML, display  \n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"credentials.env\")\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "    \n",
    "os.makedirs(\"data/books/\",exist_ok=True)\n",
    "    \n",
    "\n",
    "BLOB_CONTAINER_NAME = \"usecases\"\n",
    "BASE_CONTAINER_URL = \"https://blobstorageffpanhhmq7wy3.blob.core.windows.net/\" + BLOB_CONTAINER_NAME + \"/\"\n",
    "LOCAL_FOLDER = \"./data/usecases/\"\n",
    "\n",
    "os.makedirs(LOCAL_FOLDER,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60208725",
   "metadata": {},
   "outputs": [],
   "source": [
    "#search=\"example\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28c8adc3",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "leading zeros in decimal integer literals are not permitted; use an 0o prefix for octal integers (19802403.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    https://blobstorageffpanhhmq7wy3.blob.core.windows.net/books/StyleTTS2.pdf?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2024-09-13T16:22:30Z&st=2024-09-13T08:22:30Z&spr=https&sig=mnoIDbS7Z3ooFM%2BVbIFU9TbrIGfG14ss1Lt5HD1l3io%3D&#search=we%model%the%speech\u001b[0m\n\u001b[0m                                                                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m leading zeros in decimal integer literals are not permitted; use an 0o prefix for octal integers\n"
     ]
    }
   ],
   "source": [
    "https://blobstorageffpanhhmq7wy3.blob.core.windows.net/books/StyleTTS2.pdf?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2024-09-13T16:22:30Z&st=2024-09-13T08:22:30Z&spr=https&sig=mnoIDbS7Z3ooFM%2BVbIFU9TbrIGfG14ss1Lt5HD1l3io%3D&#search=we%model%the%speech\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "331692ba-b68e-4b99-9bae-5057da9a389d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the ENV variables that Langchain needs to connect to Azure OpenAI\n",
    "os.environ[\"OPENAI_API_VERSION\"] = os.environ[\"AZURE_OPENAI_API_VERSION\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "594ff0d4-56e3-4bed-843d-28c7a092069b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 75\n",
    "embedder = AzureOpenAIEmbeddings(deployment=os.environ[\"EMBEDDING_DEPLOYMENT_NAME\"], chunk_size=batch_size, \n",
    "                                 max_retries=2, \n",
    "                                 retry_min_seconds= 60,\n",
    "                                 retry_max_seconds= 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb87c647-158c-4f85-b569-5b9462f06c83",
   "metadata": {},
   "source": [
    "## 1 - Manual Document Cracking with Push to Vector-based Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75551868-1546-421b-a14e-e42618d88e61",
   "metadata": {},
   "source": [
    "Within our demo storage account, we have a container named `books`, which holds 5 books of different lengths, languages, and complexities. Let's create a `cogsrch-index-books-vector` and load it with the pages of all these books.\n",
    "\n",
    "We begin by downloading these books to our local machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0999e24b-6a75-4fa1-9a5f-426cf0f0bdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "books = [\"BigVGAN.pdf\", \n",
    "         \"PL-BERT.pdf\",\n",
    "         \"StarGANv2.pdf\",\n",
    "         \"StyleTTS2.pdf\"\n",
    "         ]\n",
    "\n",
    "usecases = [ '2323-EQU-Y-SA-0014_EQU design basis.PDF' , 'TR2258 Digital Factory - Information and Exchange Models.PDF' , 'TR2381 LCI Requirements Master for FPSO.PDF',\n",
    "'TR1212 SAS Operator Station HMI.PDF' ,     'TR2325 Piping Detail Standard.PDF'       ,                      'TR3032 Field instrumentation.PDF']\n",
    "\n",
    "\n",
    "import urllib.request\n",
    "import ssl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd867b2f-b5a1-443c-aa0a-ce914a66b3c9",
   "metadata": {},
   "source": [
    "Let's download the files to the local `./data/` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3554f0b7-fee8-4446-a155-5d22dc0f0888",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | 0/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://blobstorageffpanhhmq7wy3.blob.core.windows.net/usecases/2323-EQU-Y-SA-0014_EQU design basis.PDF?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2024-09-13T16:22:30Z&st=2024-09-13T08:22:30Z&spr=https&sig=mnoIDbS7Z3ooFM%2BVbIFU9TbrIGfG14ss1Lt5HD1l3io%3D\n"
     ]
    },
    {
     "ename": "InvalidURL",
     "evalue": "URL can't contain control characters. '/usecases/2323-EQU-Y-SA-0014_EQU design basis.PDF?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2024-09-13T16:22:30Z&st=2024-09-13T08:22:30Z&spr=https&sig=mnoIDbS7Z3ooFM%2BVbIFU9TbrIGfG14ss1Lt5HD1l3io%3D' (found at least ' ')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidURL\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(BASE_CONTAINER_URL \u001b[38;5;241m+\u001b[39m book \u001b[38;5;241m+\u001b[39m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBLOB_SAS_TOKEN\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      6\u001b[0m book_url \u001b[38;5;241m=\u001b[39m BASE_CONTAINER_URL \u001b[38;5;241m+\u001b[39m book \u001b[38;5;241m+\u001b[39m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBLOB_SAS_TOKEN\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 7\u001b[0m req \u001b[38;5;241m=\u001b[39m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbook_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLOCAL_FOLDER\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbook\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(req\u001b[38;5;241m.\u001b[39mtext)\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/urllib/request.py:241\u001b[0m, in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;124;03mRetrieve a URL into a temporary location on disk.\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;124;03mdata file as well as the resulting HTTPMessage object.\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    239\u001b[0m url_type, path \u001b[38;5;241m=\u001b[39m _splittype(url)\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mclosing(\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[1;32m    242\u001b[0m     headers \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39minfo()\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;66;03m# Just return the local path and the \"headers\" for file://\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;66;03m# URLs. No sense in performing a copy unless requested.\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/urllib/request.py:519\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    516\u001b[0m     req \u001b[38;5;241m=\u001b[39m meth(req)\n\u001b[1;32m    518\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n\u001b[0;32m--> 519\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[1;32m    522\u001b[0m meth_name \u001b[38;5;241m=\u001b[39m protocol\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_response\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/urllib/request.py:536\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m    535\u001b[0m protocol \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n\u001b[0;32m--> 536\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m    537\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_open\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/urllib/request.py:1391\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[0;32m-> 1391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHTTPSConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/urllib/request.py:1348\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1348\u001b[0m         \u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1349\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhas_header\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTransfer-encoding\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[1;32m   1351\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/http/client.py:1283\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\u001b[38;5;28mself\u001b[39m, method, url, body\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, headers\u001b[38;5;241m=\u001b[39m{}, \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   1281\u001b[0m             encode_chunked\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1283\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/http/client.py:1294\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccept-encoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m header_names:\n\u001b[1;32m   1292\u001b[0m     skips[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskip_accept_encoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1294\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mputrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mskips\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[38;5;66;03m# chunked encoding will happen if HTTP/1.1 is used and either\u001b[39;00m\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;66;03m# the caller passes encode_chunked=True or the following\u001b[39;00m\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;66;03m# conditions hold:\u001b[39;00m\n\u001b[1;32m   1299\u001b[0m \u001b[38;5;66;03m# 1. content-length has not been explicitly set\u001b[39;00m\n\u001b[1;32m   1300\u001b[0m \u001b[38;5;66;03m# 2. the body is a file or iterable, but not a str or bytes-like\u001b[39;00m\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;66;03m# 3. Transfer-Encoding has NOT been explicitly set by the caller\u001b[39;00m\n\u001b[1;32m   1303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent-length\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m header_names:\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;66;03m# only chunk body if not explicitly set for backwards\u001b[39;00m\n\u001b[1;32m   1305\u001b[0m     \u001b[38;5;66;03m# compatibility, assuming the client code is already handling the\u001b[39;00m\n\u001b[1;32m   1306\u001b[0m     \u001b[38;5;66;03m# chunking\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/http/client.py:1128\u001b[0m, in \u001b[0;36mHTTPConnection.putrequest\u001b[0;34m(self, method, url, skip_host, skip_accept_encoding)\u001b[0m\n\u001b[1;32m   1125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_method \u001b[38;5;241m=\u001b[39m method\n\u001b[1;32m   1127\u001b[0m url \u001b[38;5;241m=\u001b[39m url \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m-> 1128\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1130\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (method, url, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_http_vsn_str)\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encode_request(request))\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/http/client.py:1228\u001b[0m, in \u001b[0;36mHTTPConnection._validate_path\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m   1226\u001b[0m match \u001b[38;5;241m=\u001b[39m _contains_disallowed_url_pchar_re\u001b[38;5;241m.\u001b[39msearch(url)\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m match:\n\u001b[0;32m-> 1228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidURL(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mURL can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt contain control characters. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1229\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(found at least \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmatch\u001b[38;5;241m.\u001b[39mgroup()\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mInvalidURL\u001b[0m: URL can't contain control characters. '/usecases/2323-EQU-Y-SA-0014_EQU design basis.PDF?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2024-09-13T16:22:30Z&st=2024-09-13T08:22:30Z&spr=https&sig=mnoIDbS7Z3ooFM%2BVbIFU9TbrIGfG14ss1Lt5HD1l3io%3D' (found at least ' ')"
     ]
    }
   ],
   "source": [
    "ssl_context = ssl.create_default_context()\n",
    "\n",
    "\n",
    "for book in tqdm(usecases):\n",
    "    print(BASE_CONTAINER_URL + book + os.environ['BLOB_SAS_TOKEN'])\n",
    "    book_url = BASE_CONTAINER_URL + book + os.environ['BLOB_SAS_TOKEN']\n",
    "    req = urllib.request.urlretrieve(book_url, LOCAL_FOLDER+ book)\n",
    "\n",
    "    print(req.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788cc0db-9dae-45f2-8943-2b6fa32fcc75",
   "metadata": {},
   "source": [
    "### What to use: pyPDF or AI Documment Intelligence API (Form Recognizer)?\n",
    "\n",
    "In `utils.py` there is a **parse_pdf()** function. This utility function can parse local files using PyPDF library and can also parse local or from_url PDFs files using Azure AI Document Intelligence (Former Form Recognizer).\n",
    "\n",
    "If `form_recognizer=False`, the function will parse the PDF using the python pyPDF library, which 75% of the time does a good job.<br>\n",
    "\n",
    "Setting `form_recognizer=True`, is the best (and slower) parsing method using AI Documment Intelligence API (former known as Form Recognizer). You can specify the prebuilt model to use, the default is `model=\"prebuilt-document\"`. However, if you have a complex document with tables, charts and figures , you can try\n",
    "`model=\"prebuilt-layout\"`, and it will capture all of the nuances of each page (it takes longer of course).\n",
    "\n",
    "**Note: Many PDFs are scanned images. For example, any signed contract that was scanned and saved as PDF will NOT be parsed by pyPDF. Only AI Documment Intelligence API will work.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1c63a2f-7a53-4346-8a1f-483cfd159d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Text from 2323-EQU-Y-SA-0014_EQU design basis.PDF...\n",
      "Extracting text using PyPDF\n",
      "Extracting Text from TR2258 Digital Factory - Information and Exchange Models.PDF...\n",
      "Extracting text using PyPDF\n",
      "Extracting Text from TR2381 LCI Requirements Master for FPSO.PDF...\n",
      "Extracting text using PyPDF\n",
      "Extracting Text from TR1212 SAS Operator Station HMI.PDF...\n",
      "Extracting text using PyPDF\n",
      "Extracting Text from TR2325 Piping Detail Standard.PDF...\n",
      "Extracting text using PyPDF\n",
      "Extracting Text from TR3032 Field instrumentation.PDF...\n",
      "Extracting text using PyPDF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "incorrect startxref pointer(1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing TR2325 Piping Detail Standard.PDF took: 7.78 seconds\n",
      "TR2325 Piping Detail Standard.PDF contained 70 pages\n",
      "\n",
      "Parsing TR3032 Field instrumentation.PDF took: 15.08 seconds\n",
      "TR3032 Field instrumentation.PDF contained 103 pages\n",
      "\n",
      "Parsing TR2258 Digital Factory - Information and Exchange Models.PDF took: 20.29 seconds\n",
      "TR2258 Digital Factory - Information and Exchange Models.PDF contained 158 pages\n",
      "\n",
      "Parsing TR1212 SAS Operator Station HMI.PDF took: 25.50 seconds\n",
      "TR1212 SAS Operator Station HMI.PDF contained 225 pages\n",
      "\n",
      "Parsing TR2381 LCI Requirements Master for FPSO.PDF took: 26.83 seconds\n",
      "TR2381 LCI Requirements Master for FPSO.PDF contained 154 pages\n",
      "\n",
      "Parsing 2323-EQU-Y-SA-0014_EQU design basis.PDF took: 27.91 seconds\n",
      "2323-EQU-Y-SA-0014_EQU design basis.PDF contained 78 pages\n",
      "\n",
      "Total time for parsing all books: 27.91 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import concurrent.futures\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize a dictionary to store the results\n",
    "book_pages_map = dict()\n",
    "\n",
    "# Function to process each book\n",
    "def process_book(book):\n",
    "    try:\n",
    "        print(f\"Extracting Text from {book}...\")\n",
    "\n",
    "        # Capture the start time for the individual book\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Create the full path to the book\n",
    "        book_path = Path(LOCAL_FOLDER) / book\n",
    "\n",
    "        # Parse the PDF (assuming parse_pdf is a custom function)\n",
    "        book_map = parse_pdf(file=str(book_path), form_recognizer=False, verbose=True)\n",
    "\n",
    "        # Capture the elapsed time\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "        # Print the time it took to process the book and the number of pages\n",
    "        print(f\"Parsing {book} took: {elapsed_time:.2f} seconds\")\n",
    "        print(f\"{book} contained {len(book_map)} pages\\n\")\n",
    "\n",
    "        # Return the book name and the parsed result (book_map)\n",
    "        return book, book_map\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {book}: {e}\")\n",
    "        return book, None\n",
    "\n",
    "# Capture the start time for the entire process\n",
    "overall_start_time = time.time()\n",
    "\n",
    "# Process the books in parallel using ThreadPoolExecutor\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    # Map results of the parallel processing to 'results'\n",
    "    results = executor.map(process_book, usecases)\n",
    "\n",
    "# Store the results in the book_pages_map dictionary (only store successfully processed books)\n",
    "book_pages_map = {book: book_map for book, book_map in results if book_map}\n",
    "\n",
    "# Capture the end time for the entire process\n",
    "overall_end_time = time.time()\n",
    "\n",
    "# Calculate the total elapsed time for all books\n",
    "total_elapsed_time = overall_end_time - overall_start_time\n",
    "print(f\"Total time for parsing all books: {total_elapsed_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de0a722-ae0c-4b57-802a-518f5d4d93fd",
   "metadata": {},
   "source": [
    "Now let's check a random page of each book to make sure the parsing was done correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2a5d62f-b664-4662-a6c9-a1eb2a3c5e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2323-EQU-Y-SA-0014_EQU design basis.PDF\n",
      "Chunk text (from page 48): ...\n",
      "\n",
      "TR2258 Digital Factory - Information and Exchange Models.PDF\n",
      "Chunk text (from page 46): 4.5.1 Automation Engineering Evolution\n",
      "Integrated tool chains or the automated detection of the plants configuration is ...\n",
      "\n",
      "TR2381 LCI Requirements Master for FPSO.PDF\n",
      "Chunk text (from page 36): Requirements : LCI Requirements Master for FPSO  Classification: Internal  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Operation and maintenance , Techni...\n",
      "\n",
      "TR1212 SAS Operator Station HMI.PDF\n",
      "Chunk text (from page 32): critical situations. Following considerations should be made for level 1 pages if there are no large screen display as\n",
      "d...\n",
      "\n",
      "TR2325 Piping Detail Standard.PDF\n",
      "Chunk text (from page 23): SR-28925 - Drain tundish, deck mounted PDS DT01Requirements: Piping Detail Standard Classification: Internal\n",
      "Project dev...\n",
      "\n",
      "TR3032 Field instrumentation.PDF\n",
      "Chunk text (from page 19): Governing document : Field instrumentation Classification: Internal \n",
      " \n",
      " \n",
      "Project development (PD) , Technical and profes...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Loop over each book and its corresponding map\n",
    "for bookname, bookmap in book_pages_map.items():\n",
    "    try:\n",
    "        # Ensure the random index is within the bounds of the bookmap (number of pages)\n",
    "        if len(bookmap) > 1:  # Make sure there are at least 10 pages\n",
    "            random_page_index = random.randint(5, min(50, len(bookmap)-1))  # Ensure index doesn't exceed the available pages\n",
    "            \n",
    "            # Get the content of the randomly selected page\n",
    "            page_content = bookmap[random_page_index]\n",
    "\n",
    "            # Check if the page_content has at least 3 elements to safely access [2]\n",
    "            if len(page_content) > 2 and isinstance(page_content[2], str):  # Ensure there's a valid chunk at index 2\n",
    "                chunk_text = page_content[2][:120]  # Get the first 120 characters\n",
    "                print(f\"{bookname}\\nChunk text (from page {random_page_index}): {chunk_text}...\\n\")\n",
    "            else:\n",
    "                print(f\"{bookname}\\nPage {random_page_index} does not have enough content or a valid chunk at index [2].\\n\")\n",
    "        else:\n",
    "            print(f\"{bookname} does not have enough pages to select a random chunk.\\n\")\n",
    "    \n",
    "    except IndexError as e:\n",
    "        print(f\"IndexError for {bookname}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {bookname}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcdc1ee-71fc-49d2-8e7c-0964bc3a4370",
   "metadata": {},
   "source": [
    "As we can see above, all books were parsed except `Pere_Riche_Pere_Pauvre.pdf` (this book is \"Rich Dad, Poor Dad\" written in French), why? Well, as we mentioned above, this book was scanned, so each page is an image and with a very unique font. We need a good PDF parser with good OCR capabilities in order to extract the content of this PDF. \n",
    "Let's try to parse this book again, but this time using Azure Document Intelligence API (former Form Recognizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "801c6bc2-467c-4418-aa7e-ef89a1e20e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text using Azure Document Intelligence\n",
      "CPU times: user 42.9 ms, sys: 11.7 ms, total: 54.7 ms\n",
      "Wall time: 18.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "book = \"2323-EQU-Y-SA-0014_EQU design basis.pdf\"\n",
    "book_path = LOCAL_FOLDER+book\n",
    "book_map = parse_pdf(file=book_path, form_recognizer=True, model=\"prebuilt-document\",from_url=False, verbose=True)\n",
    "book_pages_map[book]= book_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97f9c5bb-c44b-4a4d-9780-591f9f8d128a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2323-EQU-Y-SA-0014_EQU design basis.pdf \n",
      " chunk text:   ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if book in book_pages_map:\n",
    "    book_map = book_pages_map[book]\n",
    "    if len(book_map) > 1:\n",
    "        random_page = random.randint(1, min(50, len(book_map)-1))  # Ensure index doesn't exceed the available pages\n",
    "        if len(book_map[random_page]) > 2 and isinstance(book_map[random_page][2], str):  # Ensure there's a valid chunk at index [2]\n",
    "            print(book, \"\\n\", \"chunk text:\", book_map[random_page][2][:80], \"...\\n\")\n",
    "        else:\n",
    "            print(f\"{book} does not have enough content or a valid chunk at index [2].\")\n",
    "    else:\n",
    "        print(f\"{book} does not have enough pages to select a random chunk.\")\n",
    "else:\n",
    "    print(f\"{book} is not defined in book_pages_map.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c279dfb-4fed-41b8-89e1-0ca2cefbcdc9",
   "metadata": {},
   "source": [
    "As demonstrated above, Azure Document Intelligence proves to be superior to pyPDF. **For production scenarios, we strongly recommend using Azure Document Intelligence consistently**. When doing so, it's important to make a wise choice between the available models, such as \"prebuilt-document,\" \"prebuilt-layout,\" or others. You can find more information on model selection [HERE](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/choose-model-feature?view=doc-intel-3.0.0).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5f9b7d-99e6-426d-a47e-343c7e8b492e",
   "metadata": {},
   "source": [
    "## Create Vector-based index\n",
    "\n",
    "\n",
    "Now that we have the content of the book's chunks (each page of each book) in the dictionary `book_pages_map`, let's create the Vector index in our Azure Search Engine where this content is going to land"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d46e7c5-49c4-40f3-bb2d-79a9afeab4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "usecase_index_name = \"srch-index-usecases\"\n",
    "books_index_name = \"srch-index-books\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b07e84b-d306-4bc9-9124-e64f252dd7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create Azure Search Vector-based Index\n",
    "# Setup the Payloads header\n",
    "headers = {'Content-Type': 'application/json','api-key': os.environ['AZURE_SEARCH_KEY']}\n",
    "params = {'api-version': os.environ['AZURE_SEARCH_API_VERSION']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faab899-977b-40d0-b36e-26f75ac07e54",
   "metadata": {},
   "source": [
    "\n",
    "Please note the following points regarding the index:\n",
    "\n",
    "- The ParentKey field is absent.\n",
    "- The page_num field is present.\n",
    "\n",
    "The absence of the ParentKey field is due to the utilization of a PUSH method, rather than a PULL method. This approach indicates that we are not leveraging the integrated indexing provided by the Azure AI Search engine. Instead, we are engaging in the process of parsing, performing OCR, and manually creating and pushing the content along with its vectors.\n",
    "\n",
    "This manual parsing process involves the use of either, the pyPDF library, or the Azure Document Intelligence API. These APIs allow for the segmentation of content by page rather than by a specified number of characters, which is the method employed by the Azure AI search indexer. Consequently, this enables the inclusion of page_num as a field in our index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d63e68-69a5-4b3b-8eb0-86da02cb7230",
   "metadata": {},
   "source": [
    "REST API version 2023-10-01-Preview supports external and internal vectorization. This Notebook assumes an external vectorization strategy. This API also supports:\n",
    "    \n",
    "- vectorSearch algorithms, hnsw and exhaustiveKnn nearest neighbors, with parameters for indexing and scoring.\n",
    "- vectorProfiles for multiple combinations of algorithm configurations.\n",
    "\n",
    "Vector search algorithms include **exhaustive k-nearest neighbors (KNN)** and **Hierarchical Navigable Small World (HNSW)**. Exhaustive KNN performs a brute-force search that scans the entire vector space. HNSW performs an approximate nearest neighbor (ANN) search. While KNN provides exact nearest neighbor search results with high accuracy, its computational cost and poor scalability make it impractical for large datasets or real-time applications. HNSW, on the other hand, offers a highly efficient and scalable solution for nearest neighbor searches by finding approximate nearest neighbors quickly, making it more suitable for large-scale and high-dimensional data applications.\n",
    "\n",
    "\n",
    "check [HERE](https://learn.microsoft.com/en-us/azure/search/vector-search-how-to-create-index?tabs=config-2023-10-01-Preview%2Crest-2023-11-01%2Cpush%2Cportal-check-index) for the details of the vector configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2df4db6b-969b-4b91-963f-9334e17a4e3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'book_index_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m index_payload \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mbook_index_name\u001b[49m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvectorSearch\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malgorithms\u001b[39m\u001b[38;5;124m\"\u001b[39m: [  \u001b[38;5;66;03m# We are showing here 3 types of search algorithms configurations that you can do\u001b[39;00m\n\u001b[1;32m      5\u001b[0m              {\n\u001b[1;32m      6\u001b[0m                  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy-hnsw-config-1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m                  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhnsw\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m                  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhnswParameters\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m      9\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m     10\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mefConstruction\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m400\u001b[39m,\n\u001b[1;32m     11\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mefSearch\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m500\u001b[39m,\n\u001b[1;32m     12\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetric\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcosine\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m                  }\n\u001b[1;32m     14\u001b[0m              },\n\u001b[1;32m     15\u001b[0m              {\n\u001b[1;32m     16\u001b[0m                  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy-hnsw-config-2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     17\u001b[0m                  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhnsw\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m                  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhnswParameters\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     19\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m     20\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mefConstruction\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m800\u001b[39m,\n\u001b[1;32m     21\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mefSearch\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m800\u001b[39m,\n\u001b[1;32m     22\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetric\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcosine\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     23\u001b[0m                  }\n\u001b[1;32m     24\u001b[0m              },\n\u001b[1;32m     25\u001b[0m              {\n\u001b[1;32m     26\u001b[0m                  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy-eknn-config\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     27\u001b[0m                  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexhaustiveKnn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     28\u001b[0m                  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexhaustiveKnnParameters\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     29\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetric\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcosine\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     30\u001b[0m                  }\n\u001b[1;32m     31\u001b[0m              }\n\u001b[1;32m     32\u001b[0m         ],\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvectorizers\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m     34\u001b[0m             {\n\u001b[1;32m     35\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     36\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkind\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mazureOpenAI\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     37\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mazureOpenAIParameters\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     38\u001b[0m                 {\n\u001b[1;32m     39\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresourceUri\u001b[39m\u001b[38;5;124m\"\u001b[39m : os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAZURE_OPENAI_ENDPOINT\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     40\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapiKey\u001b[39m\u001b[38;5;124m\"\u001b[39m : os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAZURE_OPENAI_API_KEY\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     41\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeploymentId\u001b[39m\u001b[38;5;124m\"\u001b[39m : os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEMBEDDING_DEPLOYMENT_NAME\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     42\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodelName\u001b[39m\u001b[38;5;124m\"\u001b[39m : os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEMBEDDING_DEPLOYMENT_NAME\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     43\u001b[0m                 }\n\u001b[1;32m     44\u001b[0m             }\n\u001b[1;32m     45\u001b[0m         ],\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprofiles\u001b[39m\u001b[38;5;124m\"\u001b[39m: [  \u001b[38;5;66;03m# profiles is the diferent kind of combinations of algos and vectorizers\u001b[39;00m\n\u001b[1;32m     47\u001b[0m             {\n\u001b[1;32m     48\u001b[0m              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy-vector-profile-1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     49\u001b[0m              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malgorithm\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy-hnsw-config-1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     50\u001b[0m              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvectorizer\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     51\u001b[0m             },\n\u001b[1;32m     52\u001b[0m             {\n\u001b[1;32m     53\u001b[0m              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy-vector-profile-2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     54\u001b[0m              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malgorithm\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy-hnsw-config-2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     55\u001b[0m              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvectorizer\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     56\u001b[0m             },\n\u001b[1;32m     57\u001b[0m             {\n\u001b[1;32m     58\u001b[0m              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy-vector-profile-3\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     59\u001b[0m              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malgorithm\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy-eknn-config\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     60\u001b[0m              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvectorizer\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     61\u001b[0m             }\n\u001b[1;32m     62\u001b[0m         ]\n\u001b[1;32m     63\u001b[0m     },\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msemantic\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigurations\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m     66\u001b[0m             {\n\u001b[1;32m     67\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy-semantic-config\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     68\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprioritizedFields\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     69\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitleField\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m     70\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfieldName\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m                     },\n\u001b[1;32m     72\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprioritizedContentFields\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m     73\u001b[0m                         {\n\u001b[1;32m     74\u001b[0m                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfieldName\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m                         }\n\u001b[1;32m     76\u001b[0m                     ],\n\u001b[1;32m     77\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprioritizedKeywordsFields\u001b[39m\u001b[38;5;124m\"\u001b[39m: []\n\u001b[1;32m     78\u001b[0m                 }\n\u001b[1;32m     79\u001b[0m             }\n\u001b[1;32m     80\u001b[0m         ]\n\u001b[1;32m     81\u001b[0m     },\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfields\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m     83\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEdm.String\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilterable\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m },\n\u001b[1;32m     84\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEdm.String\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msearchable\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretrievable\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     85\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEdm.String\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msearchable\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretrievable\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     86\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEdm.String\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msearchable\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretrievable\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msortable\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilterable\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacetable\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     87\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEdm.String\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msearchable\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretrievable\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msortable\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilterable\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacetable\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     88\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpage_num\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEdm.Int32\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msearchable\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretrievable\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     89\u001b[0m         {\n\u001b[1;32m     90\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunkVector\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     91\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollection(Edm.Single)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     92\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdimensions\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m3072\u001b[39m,\n\u001b[1;32m     93\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvectorSearchProfile\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy-vector-profile-3\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# we picked profile 3 to show that this index uses eKNN vs HNSW (on prior notebooks)\u001b[39;00m\n\u001b[1;32m     94\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msearchable\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     95\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mretrievable\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     96\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilterable\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     97\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msortable\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     98\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacetable\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m         }\n\u001b[1;32m    100\u001b[0m         \n\u001b[1;32m    101\u001b[0m     ],\n\u001b[1;32m    102\u001b[0m }\n\u001b[1;32m    104\u001b[0m r \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mput(os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAZURE_SEARCH_ENDPOINT\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/indexes/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m book_index_name,\n\u001b[1;32m    105\u001b[0m                  data\u001b[38;5;241m=\u001b[39mjson\u001b[38;5;241m.\u001b[39mdumps(index_payload), headers\u001b[38;5;241m=\u001b[39mheaders, params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mprint\u001b[39m(r\u001b[38;5;241m.\u001b[39mstatus_code)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'book_index_name' is not defined"
     ]
    }
   ],
   "source": [
    "index_payload = {\n",
    "    \"name\": book_index_name,\n",
    "    \"vectorSearch\": {\n",
    "        \"algorithms\": [  # We are showing here 3 types of search algorithms configurations that you can do\n",
    "             {\n",
    "                 \"name\": \"my-hnsw-config-1\",\n",
    "                 \"kind\": \"hnsw\",\n",
    "                 \"hnswParameters\": {\n",
    "                     \"m\": 4,\n",
    "                     \"efConstruction\": 400,\n",
    "                     \"efSearch\": 500,\n",
    "                     \"metric\": \"cosine\"\n",
    "                 }\n",
    "             },\n",
    "             {\n",
    "                 \"name\": \"my-hnsw-config-2\",\n",
    "                 \"kind\": \"hnsw\",\n",
    "                 \"hnswParameters\": {\n",
    "                     \"m\": 8,\n",
    "                     \"efConstruction\": 800,\n",
    "                     \"efSearch\": 800,\n",
    "                     \"metric\": \"cosine\"\n",
    "                 }\n",
    "             },\n",
    "             {\n",
    "                 \"name\": \"my-eknn-config\",\n",
    "                 \"kind\": \"exhaustiveKnn\",\n",
    "                 \"exhaustiveKnnParameters\": {\n",
    "                     \"metric\": \"cosine\"\n",
    "                 }\n",
    "             }\n",
    "        ],\n",
    "        \"vectorizers\": [\n",
    "            {\n",
    "                \"name\": \"openai\",\n",
    "                \"kind\": \"azureOpenAI\",\n",
    "                \"azureOpenAIParameters\":\n",
    "                {\n",
    "                    \"resourceUri\" : os.environ['AZURE_OPENAI_ENDPOINT'],\n",
    "                    \"apiKey\" : os.environ['AZURE_OPENAI_API_KEY'],\n",
    "                    \"deploymentId\" : os.environ['EMBEDDING_DEPLOYMENT_NAME'],\n",
    "                    \"modelName\" : os.environ['EMBEDDING_DEPLOYMENT_NAME']\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        \"profiles\": [  # profiles is the diferent kind of combinations of algos and vectorizers\n",
    "            {\n",
    "             \"name\": \"my-vector-profile-1\",\n",
    "             \"algorithm\": \"my-hnsw-config-1\",\n",
    "             \"vectorizer\":\"openai\"\n",
    "            },\n",
    "            {\n",
    "             \"name\": \"my-vector-profile-2\",\n",
    "             \"algorithm\": \"my-hnsw-config-2\",\n",
    "             \"vectorizer\":\"openai\"\n",
    "            },\n",
    "            {\n",
    "             \"name\": \"my-vector-profile-3\",\n",
    "             \"algorithm\": \"my-eknn-config\",\n",
    "             \"vectorizer\":\"openai\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"semantic\": {\n",
    "        \"configurations\": [\n",
    "            {\n",
    "                \"name\": \"my-semantic-config\",\n",
    "                \"prioritizedFields\": {\n",
    "                    \"titleField\": {\n",
    "                        \"fieldName\": \"title\"\n",
    "                    },\n",
    "                    \"prioritizedContentFields\": [\n",
    "                        {\n",
    "                            \"fieldName\": \"chunk\"\n",
    "                        }\n",
    "                    ],\n",
    "                    \"prioritizedKeywordsFields\": []\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"fields\": [\n",
    "        {\"name\": \"id\", \"type\": \"Edm.String\", \"key\": \"true\", \"filterable\": \"true\" },\n",
    "        {\"name\": \"title\",\"type\": \"Edm.String\",\"searchable\": \"true\",\"retrievable\": \"true\"},\n",
    "        {\"name\": \"chunk\",\"type\": \"Edm.String\",\"searchable\": \"true\",\"retrievable\": \"true\"},\n",
    "        {\"name\": \"name\", \"type\": \"Edm.String\", \"searchable\": \"true\", \"retrievable\": \"true\", \"sortable\": \"false\", \"filterable\": \"false\", \"facetable\": \"false\"},\n",
    "        {\"name\": \"location\", \"type\": \"Edm.String\", \"searchable\": \"false\", \"retrievable\": \"true\", \"sortable\": \"false\", \"filterable\": \"false\", \"facetable\": \"false\"},\n",
    "        {\"name\": \"page_num\",\"type\": \"Edm.Int32\",\"searchable\": \"false\",\"retrievable\": \"true\"},\n",
    "        {\n",
    "            \"name\": \"chunkVector\",\n",
    "            \"type\": \"Collection(Edm.Single)\",\n",
    "            \"dimensions\": 3072,\n",
    "            \"vectorSearchProfile\": \"my-vector-profile-3\", # we picked profile 3 to show that this index uses eKNN vs HNSW (on prior notebooks)\n",
    "            \"searchable\": \"true\",\n",
    "            \"retrievable\": \"true\",\n",
    "            \"filterable\": \"false\",\n",
    "            \"sortable\": \"false\",\n",
    "            \"facetable\": \"false\"\n",
    "        }\n",
    "        \n",
    "    ],\n",
    "}\n",
    "\n",
    "r = requests.put(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexes/\" + book_index_name,\n",
    "                 data=json.dumps(index_payload), headers=headers, params=params)\n",
    "print(r.status_code)\n",
    "print(r.ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36691ff0-c4c8-49d0-bfa8-3e076ece0ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to debug errors\n",
    "r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc7dda9-4725-410e-9465-54f0298fc758",
   "metadata": {},
   "source": [
    "## Upload the Document chunks and its vectors to the Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73e7600-7902-48d4-b199-9d9dc0a17aa0",
   "metadata": {},
   "source": [
    "The following code will iterate over each chunk of each book and use the Azure Search Rest API upload method to insert each document with its corresponding vector (using OpenAI embedding model) to the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a94911cf-c95f-4306-8574-b56296f29b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a batch of pages\n",
    "def process_batch(bookname, pages):\n",
    "    try:\n",
    "        contents = [page[2] for page in pages]\n",
    "        chunk_vectors = embedder.embed_documents(contents)\n",
    "        \n",
    "        upload_payload = {\"value\": []}\n",
    "        for i, page in enumerate(pages):\n",
    "            page_num = page[0] + 1\n",
    "            content = page[2]\n",
    "            book_url = BASE_CONTAINER_URL + bookname\n",
    "            \n",
    "            payload = {\n",
    "                \"@search.action\": \"upload\",\n",
    "                \"id\": text_to_base64(bookname + str(page_num)),\n",
    "                \"title\": f\"{bookname}_page_{str(page_num)}\",\n",
    "                \"chunk\": content,\n",
    "                \"chunkVector\": chunk_vectors[i],\n",
    "                \"name\": bookname,\n",
    "                \"location\": book_url,\n",
    "                \"page_num\": page_num\n",
    "            }\n",
    "            upload_payload[\"value\"].append(payload)\n",
    "        \n",
    "        r = requests.post(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexes/\" + book_index_name + \"/docs/index\",\n",
    "                          data=json.dumps(upload_payload), headers=headers, params=params)\n",
    "        if r.status_code != 200:\n",
    "            print(f\"Failed to upload batch of pages from {bookname}: {r.status_code}\")\n",
    "            print(r.text)\n",
    "    except Exception as e:\n",
    "        print(f\"Exception processing batch of pages from {bookname}: {e}\")\n",
    "        time.sleep(10)  # Wait before retrying\n",
    "        process_batch(bookname, pages)  # Retry the same batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793a3171-f8f0-4070-8a54-8a540828333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for bookname, bookmap in book_pages_map.items():\n",
    "        print(\"Uploading chunks from\", bookname)\n",
    "        # Split bookmap into chunks of size chunk_size\n",
    "        for i in tqdm(range(0, len(bookmap), batch_size)):\n",
    "            batch = bookmap[i:i + batch_size]\n",
    "            process_batch(bookname, batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715cddcf-af7b-4006-a047-853fc7a66be3",
   "metadata": {},
   "source": [
    "## Query the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b408798-5527-44ca-9dba-cad2ee726aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION = \"Considerations in design for future tie-in(s) and tie-back(s). whats the reservoir pressure in the cambroiol central and Cappahayden West block?\"\n",
    "QUESTION1 = \"Ablation study for verifying the effectiveness of MLM, P2G, and BERT compared to StyleTTS w/ PL-BERT.\"\n",
    "QUESTION2 = \"How did BigVGAN perform in terms of PESQ score vs WaveGlow-256?\"\n",
    "QUESTION3 = \"what is the acronym of the main point of Made to Stick book\"\n",
    "QUESTION4= \"Tell me a python example of how do I push documents with vectors to an index using the python SDK?\"\n",
    "QUESTION5 = \"who won the soccer worldcup in 1994?\" # this question should have no answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b182ade-0ddd-47a1-b1eb-2cbf435c317f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "usecases_index_name = \"srch-index-usecases\"\n",
    "books_index_name = \"srch-index-books\"\n",
    "\n",
    "indexes = [books_index_name]\n",
    "k=20 # in this index k corresponds to the top pages as well\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d50eecb2-ce26-4127-a62b-79735b937046",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = CustomAzureSearchRetriever(indexes=[usecases_index_name], topK=k, reranker_threshold=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "410ff796-dab1-4817-a3a5-82eeff6c0c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPLETION_TOKENS = 2500\n",
    "llm = AzureChatOpenAI(deployment_name=os.environ[\"GPT4_DEPLOYMENT_NAME\"], temperature=0.5, max_tokens=COMPLETION_TOKENS).configurable_alternatives(\n",
    "    ConfigurableField(id=\"model\"),\n",
    "    default_key=\"gpt35\",\n",
    "    gpt4=AzureChatOpenAI(deployment_name=os.environ[\"GPT4_DEPLOYMENT_NAME\"], temperature=0, max_tokens=COMPLETION_TOKENS),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33c2851-08c5-4e7c-ba7b-4655a6021e1e",
   "metadata": {},
   "source": [
    "In `utils.py` we created the **CustomAzureSearchRetriever** class that we will use going forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26f47c69-44d8-48e3-974e-7989b4a8b7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever, # Passes the question to the retriever and the results are assign to context\n",
    "        \"question\": itemgetter(\"question\")\n",
    "    }\n",
    "    | DOCSEARCH_PROMPT  # Passes the 4 variables above to the prompt template\n",
    "    | llm   # Passes the finished prompt to the LLM\n",
    "    | StrOutputParser()  # converts the output (Runnable object) to the desired output (string)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765df250-af7f-46c9-8d7a-15c0522969ec",
   "metadata": {},
   "source": [
    "#### With GPT 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "73f34192-519d-45b9-a0e2-a8b2de51ee1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4a8761d-2c1e-4369-b7c4-c3571a0793e9",
   "metadata": {},
   "source": [
    "#### With GPT 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14b77511-b178-4c9b-9fa5-fdddb0d3e586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reservoir pressure in the Cambriol Central is 800 bar, while in the Cappahayden West block, it is 557 bar [[1]](https://blobstorageffpanhhmq7wy3.blob.core.windows.net/usecases/2323-EQU-Y-SA-0014_EQU%20design%20basis.PDF?sv=2022-11-02&ss=bfqt&srt=c&sp=rwdlacupiytfx&se=2024-09-17T16:14:51Z&st=2024-09-17T08:14:51Z&spr=https&sig=6XkXMUFcHtAMoAds4flq5qCtDSc4%2B3j42IsJpEJu3sQ%3D).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# Retrieve the SAS token from the environment (or define it directly)\n",
    "sas_token = os.environ['BLOB_SAS_TOKEN']  # Assuming the token starts with '?'\n",
    "\n",
    "# Regex pattern to match .pdf/.PDF links (with trailing characters like ) and spaces)\n",
    "pdf_pattern = re.compile(r'(https?://[^\\s\\)]+\\.pdf)', re.IGNORECASE)\n",
    "\n",
    "# Set to track processed URLs\n",
    "processed_urls = set()\n",
    "\n",
    "# Initialize a buffer to store chunks that may contain split URLs\n",
    "buffer = ''\n",
    "\n",
    "# Process each chunk in the stream\n",
    "for chunk in chain.with_config(configurable={\"model\": \"gpt4\"}).stream(\n",
    "    {\"question\": QUESTION, \"language\": \"English\"}):\n",
    "    \n",
    "    # Append the new chunk to the buffer\n",
    "    buffer += chunk\n",
    "\n",
    "    # Use regex to find all occurrences of .pdf/.PDF links in the buffer\n",
    "    matches = pdf_pattern.findall(buffer)\n",
    "\n",
    "    #if matches:\n",
    "        #print(f\"Matches found: {matches}\")  # Debug: Print found matches\n",
    "\n",
    "    for pdf_url in matches:\n",
    "        # Check if this URL has already been processed\n",
    "        if pdf_url not in processed_urls:\n",
    "            # Append the SAS token to the URL\n",
    "            pdf_url_with_token = pdf_url  + sas_token\n",
    "\n",
    "            # Replace the URL in the buffer with the one that includes the SAS token\n",
    "            buffer = buffer.replace(pdf_url, pdf_url_with_token)\n",
    "\n",
    "            # Mark this URL as processed\n",
    "            processed_urls.add(pdf_url)\n",
    "\n",
    "            # Debug output to see what's happening\n",
    "            #print(f\"\\nFound PDF URL: {pdf_url}\\nReplaced with: {pdf_url_with_token}\\n\")\n",
    "\n",
    "    # After processing the buffer, clear it but keep the last few characters\n",
    "    # This helps in case a URL is split between chunks.\n",
    "    #print(\"Buffer Length:\", len(buffer))  # Debug: Print the buffer length\n",
    "    # Keep last 1000 characters in the buffer\n",
    "    #print(\"Buffer Length (after truncation):\", len(buffer))  # Debug: Print the truncated buffer length\n",
    "\n",
    "# Output the processed buffer\n",
    "print(buffer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3941796c-7655-4888-a358-8a62e380bd7e",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this notebook we learned how to deal with complex and large Documents and make them available for Q&A over them using [Hybrid Search](https://learn.microsoft.com/en-us/azure/search/search-get-started-vector#hybrid-search) (text + vector search).\n",
    "\n",
    "We also learned the power of Azure Document Inteligence API and why it is recommended for production scenarios where manual Document parsing (instead of Azure Search Indexer Document Cracking) is necessary.\n",
    "\n",
    "Using Azure AI Search with its Vector capabilities and hybrid search features eliminates the need for other vector databases such as Weaviate, Qdrant, Milvus, Pinecone, and so on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d9a7d1-f029-416b-8eb2-00a8afb9151d",
   "metadata": {},
   "source": [
    "# NEXT\n",
    "So far we have learned how to use OpenAI vectors and completion APIs in order to get an excelent answer from our documents stored in Azure AI Search. This is the backbone for a GPT Smart Search Engine.\n",
    "\n",
    "However, we are missing something: **How to have a conversation with this engine?**\n",
    "\n",
    "On the next Notebook, we are going to understand the concept of **memory**. This is necessary in order to have a chatbot that can establish a conversation with the user. Without memory, there is no real conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed733fb-dec4-4a8f-bff0-c7cbbcc0328e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a6e963",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py310_sdkv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
